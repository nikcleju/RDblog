<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-11-17">
<meta name="description" content="An overview of software frameworks, libraries and infrastructure for AI">

<title>My Notes - Deep Learning resources</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Notes</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deep Learning resources</h1>
                  <div>
        <div class="description">
          An overview of software frameworks, libraries and infrastructure for AI
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">software</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 17, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#high-level-frameworks" id="toc-high-level-frameworks" class="nav-link active" data-scroll-target="#high-level-frameworks">High-level frameworks</a>
  <ul>
  <li><a href="#monai" id="toc-monai" class="nav-link" data-scroll-target="#monai">Monai</a></li>
  <li><a href="#nvidia-clara" id="toc-nvidia-clara" class="nav-link" data-scroll-target="#nvidia-clara">NVIDIA Clara</a></li>
  </ul></li>
  <li><a href="#libraries" id="toc-libraries" class="nav-link" data-scroll-target="#libraries">Libraries</a>
  <ul>
  <li><a href="#rapids" id="toc-rapids" class="nav-link" data-scroll-target="#rapids">Rapids</a></li>
  <li><a href="#nvidia-triton-inference-server" id="toc-nvidia-triton-inference-server" class="nav-link" data-scroll-target="#nvidia-triton-inference-server">NVIDIA Triton Inference Server</a></li>
  <li><a href="#nvidia-deepstream" id="toc-nvidia-deepstream" class="nav-link" data-scroll-target="#nvidia-deepstream">NVIDIA DeepStream</a></li>
  <li><a href="#nvidia-tao-toolkit" id="toc-nvidia-tao-toolkit" class="nav-link" data-scroll-target="#nvidia-tao-toolkit">NVIDIA Tao Toolkit</a></li>
  <li><a href="#mevislab" id="toc-mevislab" class="nav-link" data-scroll-target="#mevislab">MeVisLab</a></li>
  <li><a href="#nvidia-digits" id="toc-nvidia-digits" class="nav-link" data-scroll-target="#nvidia-digits">NVIDIA Digits</a></li>
  <li><a href="#pytorch-tensorflow-matlab-etc" id="toc-pytorch-tensorflow-matlab-etc" class="nav-link" data-scroll-target="#pytorch-tensorflow-matlab-etc">PyTorch, Tensorflow, Matlab etc</a></li>
  </ul></li>
  <li><a href="#infrastructure" id="toc-infrastructure" class="nav-link" data-scroll-target="#infrastructure">Infrastructure</a>
  <ul>
  <li><a href="#docker-containers" id="toc-docker-containers" class="nav-link" data-scroll-target="#docker-containers">Docker containers</a></li>
  <li><a href="#jupyter-hub-server" id="toc-jupyter-hub-server" class="nav-link" data-scroll-target="#jupyter-hub-server">Jupyter Hub server</a></li>
  <li><a href="#nvidia-ngc-catalog" id="toc-nvidia-ngc-catalog" class="nav-link" data-scroll-target="#nvidia-ngc-catalog">NVIDIA NGC Catalog</a></li>
  <li><a href="#misc" id="toc-misc" class="nav-link" data-scroll-target="#misc">Misc</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="high-level-frameworks" class="level2">
<h2 class="anchored" data-anchor-id="high-level-frameworks">High-level frameworks</h2>
<p>Frameworks coming with lots of pre-built tools and algorithms.</p>
<section id="monai" class="level3">
<h3 class="anchored" data-anchor-id="monai">Monai</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Monai_arch_modules.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Monai modules</figcaption><p></p>
</figure>
</div>
<ul>
<li><a href="https://monai.io/">Home</a></li>
<li>Comprehensive framework for Medical Imaging</li>
<li>Based on PyTorch</li>
<li>Three packages:
<ul>
<li>Label: for labeling and user input. For medical experts.</li>
<li>Core: AI models and training. For researchers.</li>
<li>Deploy: Packaging, deployment, running</li>
</ul></li>
</ul>
<hr>
</section>
<section id="nvidia-clara" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-clara">NVIDIA Clara</h3>
<p><img src="img/Clara-monai.PNG" class="img-fluid"></p>
<ul>
<li><a href="https://developer.nvidia.com/industries/healthcare">Home</a></li>
<li>AI platform for medical applications, from NVIDIA, with tools and pre-trained models</li>
<li>Available as container in <a href="https://catalog.ngc.nvidia.com/?filters=&amp;orderBy=scoreDESC&amp;query=clara">NGC Catalog</a></li>
<li>Applications:
<ul>
<li>genomics</li>
<li>natural language processing (NLP)</li>
<li>imaging</li>
<li>medical devices</li>
<li>drug discovery</li>
<li>smart hospitals</li>
</ul></li>
<li>Multiple modules:
<ul>
<li>Clara Parabricks: for genomics</li>
<li>Clara Train SDK: for AI models, training, pre-trained etc. Based on Monai</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="libraries" class="level2">
<h2 class="anchored" data-anchor-id="libraries">Libraries</h2>
<p>Smaller, more targeted libraries.</p>
<section id="rapids" class="level3">
<h3 class="anchored" data-anchor-id="rapids">Rapids</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Rapids.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">rapids.ai</figcaption><p></p>
</figure>
</div>
<ul>
<li><a href="https://rapids.ai/">Home</a></li>
<li>Accelarate generic Machine Learning algorithms in Python, with minimal code changes, based on Cuda.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Rapids_Github.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Github repos of Rapids</figcaption><p></p>
</figure>
</div>
<hr>
</section>
<section id="nvidia-triton-inference-server" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-triton-inference-server">NVIDIA Triton Inference Server</h3>
<p><img src="img/nvidia-triton-scalable-diagram.svg" class="img-fluid"></p>
<ul>
<li><p>Front-end ynference server to serve models remotely</p></li>
<li><p><a href="https://developer.nvidia.com/nvidia-triton-inference-server">Home</a></p></li>
<li><p>Supports model backends from <a href="https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton">various frameworks</a>: TensorRT, ONNX-Runtime, Tensorflow, PyTorch, OpenVino, pure Python, DALI, FIL (tree-based standard ML models)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/TritonArch.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Triton diagram</figcaption><p></p>
</figure>
</div></li>
<li><blockquote class="blockquote">
<p>The new Forest Inference Library (FIL) backend in Triton provides support for high-performance inference of tree-based models with explainability (SHAP values) on CPUs and GPUs. It supports models from XGBoost, LightGBM, scikit-learn RandomForest, RAPIDS™ cuML RandomForest, and others in Treelite format.</p>
</blockquote></li>
</ul>
<hr>
</section>
<section id="nvidia-deepstream" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-deepstream">NVIDIA DeepStream</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/DS_overview_graph_architecture.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">DeepStream architecture</figcaption><p></p>
</figure>
</div>
<blockquote class="blockquote">
<p>DeepStream SDK is a complete streaming analytics toolkit based on GStreamer for AI-based multi-sensor processing, video, audio, and image understanding</p>
</blockquote>
<ul>
<li>AI toolkit for applications with streaming data: video, audio</li>
<li>Based on GStreamer libraries, integrates with GStreamer plugins</li>
<li>C++ and Python</li>
<li>Can be used on <strong>Jetson</strong> edge devices (I worked personally on this)</li>
</ul>
<hr>
</section>
<section id="nvidia-tao-toolkit" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-tao-toolkit">NVIDIA Tao Toolkit</h3>
<p><img src="img/TaoToolkit.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>The NVIDIA TAO Toolkit, built on TensorFlow and PyTorch, is a low-code version of the NVIDIA TAO framework that accelerates the model training process by abstracting away the AI/deep learning framework complexity. The TAO Toolkit lets you use the power of transfer learning to fine-tune NVIDIA pretrained models with your own data and optimize for inference—without AI expertise or large training datasets</p>
</blockquote>
<hr>
</section>
<section id="mevislab" class="level3">
<h3 class="anchored" data-anchor-id="mevislab">MeVisLab</h3>
<p><img src="img/FME_mevislab_macosx_wp_B.jpg" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Modular framework for image processing research and development with a special focus on medical imaging.</p>
<p>It allows fast integration and testing of new algorithms and the development of clinical application prototypes.</p>
</blockquote>
<ul>
<li><p><a href="https://www.mevislab.de/">Home</a></p></li>
<li><p>SW library and IDE</p></li>
<li><p>Details:</p>
<blockquote class="blockquote">
<p>MeVisLab is a rapid prototyping and development platform for medical image processing and visualization. With its image processing library, it fulfills the following requirements:</p>
<ul>
<li>Able to handle large, six-dimensional images (x, y, z, color, time, user-defined).</li>
<li>Offers easy ways to develop new algorithms or changing/improving existing ones in a modular C++ interface, perfect for a fast-developing research area.</li>
<li>Offers easy ways of combining algorithms to algorithm pipelines and networks.</li>
<li>Fast and easy integration into clinical environments due to standard interfaces, for example to DICOM.</li>
<li>Fair performance for clinical routine due to a page-based, demand-driven approach in the image processing.</li>
</ul>
<p>Beside general image processing algorithms and visualization tools, MeVisLab includes advanced medical imaging modules for segmentation, registration, volumetry and quantitative morphological, and functional analysis.</p>
<p>Based on MeVisLab, several clinical prototypes have been developed, including software assistants for neuro-imaging, dynamic image analysis, surgery planning, and vessel analysis.</p>
<p>The implementation of MeVisLab makes use of a number of well known third-party libraries and technologies, most importantly the application framework Qt, the visualization and interaction toolkit Open Inventor, the scripting language Python, and the graphics standard OpenGL. In addition, modules based on the Insight ToolKit (ITK) and the Visualization ToolKit (VTK) are available.</p>
</blockquote></li>
</ul>
<hr>
</section>
<section id="nvidia-digits" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-digits">NVIDIA Digits</h3>
<p><img src="img/digits-image1.png" class="img-fluid"></p>
<ul>
<li>Interactive Deep Learning GPU Training System</li>
<li><a href="https://developer.nvidia.com/digits">Home</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/digits/digits-tutorial/index.html">Technical Documentation</a></li>
<li>About:</li>
</ul>
<blockquote class="blockquote">
<ul>
<li>DIGITS is <strong>not a framework</strong></li>
<li>DIGITS is a <strong>wrapper for NVCaffe and TensorFlow</strong>, which provides a graphical web interface to those frameworks rather than dealing with them directly on the command-line</li>
<li>Design, train and visualize deep neural networks for image classification, segmentation and object detection using Caffe, Torch and TensorFlow</li>
<li>Pre-trained models from the DIGITS Model Store</li>
<li>Perform hyperparameter sweep of learning rate and batch size</li>
<li>Schedule, monitor, and manage neural network training jobs, and analyze accuracy and loss in real time</li>
<li>Import a wide variety of image formats and sources with DIGITS plug-in</li>
<li>Scale training jobs across multiple GPUs automatically</li>
</ul>
</blockquote>
<ul>
<li><p>Available in NGC: <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/digits">https://catalog.ngc.nvidia.com/orgs/nvidia/containers/digits</a></p></li>
<li><p><strong>Notes</strong></p>
<ul>
<li>Rather old (started in 2015), might be out of touch with recent developments</li>
<li>Last container version on NGC is from September 2021</li>
</ul></li>
</ul>
<hr>
</section>
<section id="pytorch-tensorflow-matlab-etc" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-tensorflow-matlab-etc">PyTorch, Tensorflow, Matlab etc</h3>
<ul>
<li>All available on the <a href="https://catalog.ngc.nvidia.com/">NGC Catalog</a></li>
</ul>
<hr>
</section>
</section>
<section id="infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="infrastructure">Infrastructure</h2>
<p>Low-level technology to make all the magic run</p>
<section id="docker-containers" class="level3">
<h3 class="anchored" data-anchor-id="docker-containers">Docker containers</h3>
<ul>
<li><p><a href="https://www.docker.com/resources/what-container/">What is a container</a></p></li>
<li><p>In practice, mostly used for deployment / builing in isolated environments, but not necessarily day-to-day development</p></li>
<li><p><a href="https://www.weave.works/blog/a-practical-guide-to-choosing-between-docker-containers-and-vms">Docker vs Virtual Machine</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/containers-vs-virtual-machines.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Docker vs.&nbsp;Virtual machines</figcaption><p></p>
</figure>
</div></li>
</ul>
<hr>
</section>
<section id="jupyter-hub-server" class="level3">
<h3 class="anchored" data-anchor-id="jupyter-hub-server">Jupyter Hub server</h3>
<p><img src="img/JUPYTER_ARCHITECTURE.png" class="img-fluid"></p>
<ul>
<li><a href="https://jupyterhub.readthedocs.io/en/latest">Home</a></li>
<li>Enabling multiple users to work simultaneously on their notebooks, from client browsers</li>
</ul>
<hr>
</section>
<section id="nvidia-ngc-catalog" class="level3">
<h3 class="anchored" data-anchor-id="nvidia-ngc-catalog">NVIDIA NGC Catalog</h3>
<ul>
<li>Catalog of GPU-enabled Docker containers for AI</li>
<li><a href="https://catalog.ngc.nvidia.com/">Home</a></li>
<li>Popular container collections:
<ul>
<li><p><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/nvidiaai">Nvidia AI</a> collection:</p>
<blockquote class="blockquote">
<ul>
<li>Deep Learning Frameworks: Updated monthly, PyTorch and TensorFlow</li>
<li>RAPIDS: Accelerates end-to-end data science and analytics pipelines entirely on GPUs.</li>
<li>TensorRT: Takes a trained network and produces a highly optimized runtime engine that performs inference for that network.</li>
<li>TAO: A python-based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. Add all 3 TAO containers in the entities tab.</li>
<li>Triton: An open-source software to deploy trained AI models from any framework, on any GPU- or CPU-based infrastructure in the cloud, data center, or embedded devices.</li>
<li>DeepStream: This SDK delivers a complete streaming analytics toolkit for AI based video and image understanding and multi-sensor processing.</li>
<li>RIVA: A GPU-accelerated SDK for building speech applications that are customized for your use case and deliver real-time performance. Include RIVA Clients and RIVIA Speech skills</li>
</ul>
</blockquote></li>
</ul></li>
</ul>
<hr>
</section>
<section id="misc" class="level3">
<h3 class="anchored" data-anchor-id="misc">Misc</h3>
<ul>
<li>Google Colab notebooks on local runtime container:
<ul>
<li><a href="https://towardsdatascience.com/create-your-own-gpu-accelerated-yupyter-notebook-server-with-google-colab-using-docker-2fa14900bab5">Create your own GPU accelerated Jupyter Notebook Server for Google Colab using Docker</a></li>
<li><a href="https://hub.docker.com/r/sorokine/docker-colab-local">Jupyter local runtime for Google Colab with CUDA and BERT</a></li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>